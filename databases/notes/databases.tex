\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{mathtools}

\usepackage{geometry}

\usepackage{multicol}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\geometry{margin=0.6in}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlength{\parindent}{0em}
\setlength{\parskip}{1em}


\begin{document}
\section{Database management systems}

Databases are developed in a way to grant data independence, integrity and security, efficient and concurrent
access and reduced application development time.

\subsection{Data models}
A data model are the concepts used to describe data. A schema is the description of a particular 
collection of data.\\The most used model today is the relational model. It bases its self on the concept 
of relations, and every relation has a schema

\subsubsection{Abstraction}
Data is defined in different schemas. The physical schema represents how the data is stored on the disk,
the conceptual schema defines the logical structure, and a view is how users see the data.

Schemas are defined using DDL (Data Definition Language) and the data is modified using DML (Data 
Manipulation Language)

\subsection{ACID}
ACID is the list of properties that a database must ensure. To explaine them we use the example of a 
bank transaction

\subsubsection{Atomicity}
Atomicity is a property caracterized by the all-or-nothing policy. It is applied through a log who keeps

\subsubsection{Consistency}
The database must be consistent before and after the transaction

\subsubsection{Isolation}
multiple transactions occur independently without interference

\subsubsection{Durability}
THe changes of a successful transaction occurs even if the system failure occurs

\newpage
\section{E/R Model}

\subsection{Entity}
an entity is a real world object which is described using a set of attributes. A collection of similar
entities is a entity set

\subsubsection*{weak entities}
This are entities that depend to another entity. The weak entity doesn't make sense to exist by itself. 
This entities are rappresented through a thick line around the entity and they use a thick arrow towards
the entity they depend

\subsection{Relations}
It is a connetion between two or more entity sets. \\The relation can use different multiplicity, which
are many to many, many to one and one to one. We use the wisconsin representation system, which is 
structured like this:\\ we use an arrow to point to "one", a think arrow to represent "exactly one", a 
line to represent "mmany to many" and a thick line for "at least one"

\subsubsection{Aggregation}
is used when we have to model a relationship involving entity sets and a relationship set

\subsection{Subclasses}
it's a special case of subentity, it's represented with a triangle

\subsubsection*{E/R Subclasses}
Subclasses form a tree of one-one inheritance that use isa relationships. 

\subsection{Keys}
This are unique identifiers of an entity and are represented through an underlined attribute. They can
be super keys (keys in general), candidate key (minimal super key) and primary key (the chosen key)

\subsection{Schemas}

\newpage
\section{Relational model}
it's the most used model thanks to the characteristic to be easier and faster than other models.

\subsubsection*{Definitions}
a relational database is a set of relations, which are made of instances, or tables, and schemas, which specifies
name of the relation, and name plus type of each column. In the tables, rows equals to the cardinality and are 
called tuples,the fields are called degrees

\subsection{Integrity constraint}
A condition must be true for any instance of the database.

Keys can be constraint because no distinct tuples can have the same values 

\newpage
\section{Relational algebra}
The manipulation and retrieval of data from a database is managed through query language. A mathematical query
language i the relation algebra, an operational query language.

The basic operations are:
\begin{itemize}
    \item Selection ($\sigma$): selects a subset of rows from relation 
    \item Projection ($\pi$): selects the columns from relation
    \item Cross-product ($\times$): allows to combine two relations
    \item Set difference (-) 
    \item Union ($\cup$)
\end{itemize}
there are also other sub-operators:
\begin{itemize}
    \item Renomination ($\rho$, $\rightarrow$)
    \item Join ($\Join$) or theta join: the union on determined conditions
    \item Equijoin: a join on only equalities
    \item Natural join: is a equijoin on all common fields
\end{itemize}

One complex operation is the division, which aswers questions in the format of "find ALL ... that ...",
and it follows this concept:
\begin{center}
    $A/B$ contains all $x$ tuples such that for every $y$ tuple in $B$, there is a $xy$ tuple in A\\
    OR\\
    For $A/B$, compute all $x$ values that are not `disqualifiedâ€™ by some $y$ value in $B$

    $\pi_{x}(A)\ -\ \pi_{x}((\pi_{x}(A)\times B)-A)$
\end{center}
 

\newpage
\section{SQL}
A basic SQL query is composed by relation-list, target-list and qualification and it follows this strategy:
\begin{itemize}
    \item Compute the cross-product of relation-list (FROM)
    \item discard resulting tuples if they fail qualifications (SELECT)
    \item delete attributes that are not in target-list (WHERE)
\end{itemize}
SQL is also capable of nesting queries. This is done by adding the operator IN in the WHERE section, and the nested 
query inside. It can be also used the operator EXISTS to let the inner query use parameters from the outer query. 
parentesis

NB: "at least one" equals to \texttt{ distinct}

\subsubsection{Operators LIKE and AS}
LIKE is used in string matching, it uses \% to represent multiple characters and \_ just for one. To generate new
name fields is also used the operator AS

fast renomination like \texttt{FROM table t} is using what are called range variables

To compare a field with a set of values is used \texttt{[operation] ANY/ALL(query)}. You can also check if a 
tuple is inside another query with \texttt{WHERE x IN/EXISTS}

\subsubsection{UNION and INTERSECT} 
UNION can be used to unite two compatible sets of tuples, meanwhile INTERSECT computes the intersection between the
tuples

NB: \texttt{UNIQUE} is used for "exactly one"

\subsubsection{Agregate Operators}
SQL offers various extentions to relational algebra, such as COUNT, SUM, AVG, MAX and MIN

NB: \texttt{HAVING} allowes to add other constraints on the grouping operators, it can be completed with
\texttt{EVERY}

\subsubsection{Limitations}
it can be used, during the table creation, the operator \texttt{CONSTRAINTS/CHECK} to apply constraints when
adding data to a table. To manage constraints that work between table is used \texttt{CREATE ASSERTION...}.
Also \texttt{TRIGGER} can be created

\subsection{Deductive Databases}
TO apply recursevly

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Indexing}
On datastorage, Disk can retrieve random pages at fixed cost, but must be organazied. This is called file
organization, or the mothod of arranging a file of records. Indexes are the data structures that allow to 
find records ids.Indexes are organized through trees or hashing.\\Alternatively, other file organizations 
can be heap files or sorted files.

An index contains a collection of data entries $x*$ with a key value of $x$. Any field of a relation can be the
search key for an index.

\subsubsection{B+ tree indexes}
Data entries are contained in the leaves of a tree and are chained to one another. Non-leaf pages have index 
entries used to direct searches

\subsubsection{Hash-based indexes}
Better suited for equality selection, this indexes are a collection of buckets (primary pages with zero or more
overflow pages) which contain data entries. It's also implemented an hashing function that retrieves the buckets
where a record belongs. 

\subsection{Alternatives for data entry $x$ in index}
we can store the data record with key value $x$, rids of data record or list of rids of data records.

In the first alternative, the index structure is a file organization of data records. At most one index on a 
collection of data records can use this alternative, otherwise the data is duplicated. If data records are 
very large, the number of pages is high, this implies also lage size of the auxiliary information

With the other alternatives data entries are much smaller than data records, so more efficient for large data 
records. The third alternative is even more compact

\subsection{Clustered and unclustered indexes}
An index can be categorized in primary vs secondary, and clustered vs unclustered. These second classification
differs for a sorted or unsorted heap file with all the data records

\newpage
\section{Cost Teory}
Definitions:
\begin{itemize}
    \item Page: Is the size of one unit inside a hard drive. It's denoted with $p$
    \item Record size: is the size of a tuple, denoted with $t_{R}$
    \item Pages of relations: is the number of pages occupied by the records of a single relation. it's denoted with $P_{R}$
    \item Cardinality of a relation: is the number of records of a single cardinality, is denoted with $|R|$
    \item Cardinality of an attribute: is the number of distinct values inside an attribute. Is denoted with $R.A$
\end{itemize}

This definitions can be used to determin other data

\begin{center}
    \# Records of $R$ per page:\\ $\lfloor \frac{P}{t_{R}}\rfloor = \lfloor \frac{PageSize}{t_{TupleSize}}\rfloor$

    Size of R:\\ $\ceil{\frac{|R|}{\lfloor\frac{P}{t_{R}}\rfloor}} \times p = \ceil{\frac{\# Tuples}{\# TuplesPerPage}} \times p$
\end{center}

\subsection{Cost}
Every action of I/O on the database requires time, by definitiono we will say that an action of writing and reading will cost 1.
The other tipes of action are:

\subsubsection*{Scan}
Is the operation of reading a relation, his cost is equal to the pages of a relation

\subsubsection*{Sorting}
The cost of sorting the data depends on the number of memory buffers that the database can use. If only 3 buffers are avaiable 
the cost is equal to:
\begin{center}
    $2\times N\times (\lceil log_{2}N\rceil+1)$
\end{center}
where $N$ is the number of pages that a relation occupies.\\In case more than 3 buffers are avaiable then the cost is
\begin{center}
    $2\times N\times (\lceil log_{B-1}\lceil\frac{N}{B}\rceil\rceil+1)$
\end{center}
where $B$ is the number of buffers avaiable

\subsubsection*{Index lookup}
the cost of using an index is denoted with $L$, and is equal to 1.2 in a hash index, meanwhile, for a B-tree the cost equals 
to $log_{s}|R.A|$, where $s$ equals to the maximum amount of children a node can have.

The retrieval of unclustered data equals to $L + |R_{A=x}|$, meanwhile the retrieval of clustered data is 
$L + \frac{|R_{A=x}|}{\lfloor\frac{P}{t_{R}}\rfloor}$ or $L + \# pages\ occupied\ by\ the\ tuples$

\subsubsection*{Selectivity factor}
This is the amount of records expected to respect a given condition. These records are denoted as $Rc$. The selectivity
factor is denoted as:
\begin{center}
    $f = \frac{|R_{A=x}|}{|R|}$ 
\end{center}

\subsubsection*{Update}
The cost equals to the index lookup plus the reading and writing of the page, or 
\begin{center}
    $L + \# pages\ with\ the\ updated\ records\times 2$    
\end{center}

\subsection{Joins}
We will use the relation $R$ and $S$
\subsubsection*{Nested loop join}
The cost equals to $P_{R}+P_{S}\times P_{R}$

\subsubsection{Sort Merge Join}
The cost equals to the Cost of sorting $R$ plus the cost of sorting $S$ plus $P_{R}+P_{S}$

\subsubsection*{Hash join}
This join uses a hash map to make the action more efficient. The cost equals to $3\times(P_{R}+P_{S})$

\subsubsection*{Index nested loops join}
This method uses the index of the second relation to make the join more efficient, if the index doesn't exist, then
this method is not usable. The cost equals to:
\begin{center}
    $P_{R}+|R|\times$(index lookup cost + cost of retrieveng the qualifying records)
\end{center}


\subsection{Query plans}
A query can be converted in tree composed of relational algebra operations, which is used to compute in advance the 
cost of the query. Every query can generate different trees, so it's the database role to determine the most 
efficient. This action is called optimization.

Some operations don't need to wait for the final result, but they can use records as they arrive. This operations are
called \textbf{On-the-fly} operations. Other operations don't even need to use records but only their indexes, so 
they are called \textbf{Index-only} operations


\newpage
\section{Functional dependencies and normal forms}
\subsection{Functional Dependencies}
In a relation, it happens frequently that some information is repeated in a pattern. In this case we say a 
specific attribute functionally determines the values of other attributes. This constraints are called functional 
dependecies.

More specifically, a functional dependency is expressed as $X\to Y$ if $X,Y\subseteq U$ and if 
$t_{1}[X]=t_{2}[X] \Rightarrow t_{1}[Y]=t_{2}[Y]$

\subsection{Decomposition}
Functional dependencies help identify redundancy of information. To reduce redundancy, a relation can be decomposed
in two different relations, which are then reunited with a join. This can not be always be applied because some 
relations can lead to a lossy decomposition.\\We say that a decomposition is a lossless decomposition when there 
is no loss of information when replacing the relation
\begin{center}
    $<R(U),F>\ \Rightarrow\ <R_{1}(X_{1}),F_{1}>,...,<R_{n}(X_{n}),F_{n}>$\\$U=X_{1}\cup ...\cup X_{n}$
\end{center}

Decomposition is not always more efficient, to properly apply a decomposition it must respect this properties:
\begin{itemize}
    \item The decomposition must be lossless
    \item redundancies must be eliminated
    \item The functional dependencies of the original schemas should be preserved
\end{itemize}

This property are all respected in the schema normal form

\subsubsection*{Closure of a set of functional dependencies}
Given a set of functional dependencies, some are logically implied. We say that a set $F$ of FD logically implies 
a FD $X\to Y$ if every instance that satisfies $F$ also satisfies $X\to Y$. The set of all FD implied by $F$ is the
closure of $F$, denoted as $F^{+}$

To compute the closure of a set, we use the Armstrong's Axioms:
\begin{itemize}
    \item \textbf{Reflexive rule}: if $Y\subseteq X$, then $X\to Y$
    \item \textbf{Augmentation rule}: if $X\to Y$, then $XZ\to YZ$
    \item \textbf{Transitivity rule}: if $X\to Y$ and $Y\to Z$, then $X\to Z$
    \item \textbf{Union rule}: if $X\to Y$ and $X\to Z$, then $X\to YZ$
    \item \textbf{Decomposition rule}:if $X\to YZ$, then $X\to Y$ and $X\to Z$
\end{itemize}

\subsubsection*{Closure of a set of attributes}
Denoted as $(X)^{+}$ is the set of all attributes that can be determined by $X$. This can be used to determine
if $X$ si a superkey, a minimal key or nothing at all, if it respects the property $(X)^{+}=U.X \to U$

NB: Some dependencies are defined as trivial when $X\to Y,\ Y\subseteq X$

\subsection{Normal Form}
The normal form of a relation is determined when all dependencies $X\to Y$ in $F^{+}$ are listed, with the exception
of trivial dependencies ($X$ is superkey of $R$)

\subsubsection*{Minimal cover}
A simpler version to computing the closure of $F$ is computing the minimal cover. The minimal cover follows this
properties: 
\begin{itemize}
    \item $F^{+}_{min}=F^{+}$
    \item all FD in $F_{min}$ are of form $X\to A$
    \item if we modify $F_{min}$, then $F^{+}_{min}\neq F^{+}$
\end{itemize}

To compute a minimal cover we follow these steps:
\begin{itemize}
    \item Normalize each $X\to ABC...$ to $X\to A$, $X\to B$,...
    \item if $XA\to B$ and $A\in (X)^{+}$, then is redundant and can be removed
    \item remove the remaining redundancies
\end{itemize}

\subsubsection*{Algoritm for normal form decomposition}
\begin{itemize}
    \item Choose some FD that violates the NF conditions
    \item Compute $Y=(X)^{+}/X$ and $Z=U/XY$
    \item contruct two relation schemas
    \subitem $<R_{1}(XY),(\Pi_{XY}F^{+})_{min}>,<R_{2}(XZ),(\Pi_{XZ}F^{+})_{min}>$
    \item if not in normal form, then decompose again
\end{itemize}

\subsubsection*{Third normal form}
If when for all $\alpha\to\beta$ in $F^{+}$ at least one of the following properties holds:
\begin{itemize}
    \item $\alpha\to\beta$ is trivial
    \item $\alpha$ is superkey of $R$
    \item each attribute $A$ in $\beta-\alpha$ is contained in a candidate key for $R$
\end{itemize}
then the relation is in third normal form

\subsubsection*{Cover}
$G$ is a cover of $F$ when $F^{+}=G^{+}$. When is composed of only elementary FDs is called canonical cover

\end{document}

