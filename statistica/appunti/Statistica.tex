\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fixltx2e}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\title{Statistica}
\date{}
\author{}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\def\SPSB#1#2{\rlap{\textsuperscript{#1}}\SB{#2}}
\def\SP#1{\textsuperscript{#1}}
\def\SB#1{\textsubscript{#1}}

\def\separator{\begin{center}    \rule{100pt}{0.5pt}\end{center}}

\begin{document}

\section{Introduzione}

probabilità $\rightarrow$ misurare l'incertezza\\
statistica:
\begin{itemize}
    \item descrittiva
    \item differenziale $\rightarrow$  \underline{campione casuale} per \underline{stimare} un esito
\end{itemize}

\separator

probabilità:  
\begin{center}
    $\frac{casi favorevoli}{casi totali}$
\textbf{\underline{SE}} equiprobabili
\end{center}

per contare i casi ci si appoggia alla \underline{combinatoria}

\separator

partizione: separazione di A in sottoinsiemi senza elementi comuni

\separator

\textbf{NB:} 
\begin{itemize}
    \item $\wedge$ - and $\rightarrow$ A$\cap$B = \{x$|$x$\in$A$\wedge$x$\in$B\}
    \item $\vee$ - or $\rightarrow$ A$\cup$B = \{x$|$x$\in$A$\vee$x$\in$B\}
\end{itemize} 


\textbf{Principi della combinatoria:}
\begin{enumerate}
    \item A insieme, \{E\textsubscript{i}\}\SPSB{n}{i=1} partizione di A $\rightarrow$ \#A = $\sum_{i = 1}^{n}$ 
    \#E\textsubscript{i} 
    \begin{itemize}
        \item A,B insiemi, AxB è l'insieme di coppie ordinate (a,b)
    \end{itemize}  
    \item \#(AxB) = \#A$\cdot$\#B $\rightarrow$ \{A\textsubscript{i}\}\SPSB{n}{i = 1} = $\bigotimes$\SPSB{n}{i=1} A\SB{i}
    \item A,B, \#(A$\cup$B) = \underline{\#A + \#B - \#(A{$\cap$}B)} (non perfetto) $\rightarrow$
    \begin{center}
        \item \# $\cup$\SPSB{n}{i= 1} A\SB{i} = $\sum_{i = 1}^{n}$ \#A\SB{i} - $\sum_{i<j}^{}$\#(A\SB{i}$\cap$A\SB{j}) +
        $\sum_{i<j<k}^{}$\#(A\SB{i}$\cap$A\SB{j}$\cap$A\SB{k}) + \dots
        
        $\downarrow$

        \textbf{+(-1)\SP{n+1} \#$\cap$\SPSB{n}{i=1}A\SB{i}}
    \end{center}
\end{enumerate}




\section{Permutazioni e anagrammi}

Fattoriale $\rightarrow$ x! = 9! = 9$\cdot$8$\cdot$7$\cdot$6$\cdot\dots\cdot$2$\cdot$1

\textbf{NB:} 0! = 1

\begin{itemize}
    \item "prendiamo" ha 9! anagrammi
    \item "anagramma" ha tre ripetizioni di a e due ripetizioni di m, quindi per calcolare i casi unici:   
\end{itemize}
\begin{center}
    \Large $\frac{9!}{3!{\cdot}2!}$ \normalsize

    $\downarrow$
 \end{center} 
per calcolare la probabilitàdegli elementi n, ma mi interessano solo k elementi allora:
\begin{center}
    \Large $\frac{n!}{(n-k)!}$ \normalsize
\end{center}
se non sono interessato all'ordine, allora:
\begin{center}
    \Large $\frac{n!}{(n-k)!k!}$ $\Rightarrow$ $\binom{n}{k}$ \normalsize
\end{center}
chiamato anche \textbf{coefficente binominiale}

\separator

\textbf{Proprietà:}
\begin{itemize}
    \item $\binom{n}{k}$ = $\binom{n}{n-k}$
    \item $\binom{n}{0}$ = $\binom{n}{n}$ = 1
    \item $\sum_{k=0}^{n}$ $\binom{n}{k}$  = 2\SP{n}
    \item $\binom{n}{k}$ + $\binom{n}{k+1}$ = $\binom{n+1}{k+1}$  
\end{itemize}




\section{Esperimenti aliatori}

Un esperimento si definisce \textbf{aliatorio} o casuale se con i dati iniziali il risultato è incerto.
I risultati a 2 a2 incompatibili di un esperimento aliatorio sono chiamati \textbf{esiti}.
$\Omega$ denota lo \textbf{spazio degli esiti}.
Un \textbf{evento} è un osservabile di un esperimento aliatorio.

Una parte di $\Omega$ può essere considerata come famiglia:
\begin{center}
    $\mathcal{F}$ $\subseteq$ P($\Omega$)
\end{center}
Questa è definita come \textbf{algebra} se:
\begin{itemize}
    \item $\Omega\in\mathcal{F}$
    \item se A$\in\mathcal{F}$ allora A\SP{c}$\in\mathcal{F}$
    \item se A,B$\in\mathcal{F}$, allora A$\cup$B$\in\mathcal{F}$
    \subitem potremmo scrivere anche \{A\SB{i}\}\SPSB{n}{i=1}$\subseteq\mathcal{F}$ allora 
    $\cup$\SPSB{n}{i=1}A\SB{i}$\in\mathcal{F}$
\end{itemize}

\separator

\large{\textbf{Proprietà}}\normalsize
\begin{itemize}
    \item $\emptyset\in\mathcal{F}$
    \item se A,B$\in\mathcal{F}$ allora A$\cap$B$\in\mathcal{F}$
    \item se \{A\SB{i}\}\SPSB{n}{i=1}$\subseteq\mathcal{F}$ allora $\cap$\SPSB{n}{i=1}A\SB{i}$\in\mathcal{F}$
    \item se A,B $\in\mathcal{F}$, allora A$\cdot$B $\in\mathcal{F}$
    \item se A,B $\in\mathcal{F}$, allora A$\triangle$B $\in\mathcal{F}$
\end{itemize}

\separator
 
$\mathcal{F}\subseteq$P($\Omega)$ è una \textbf{tribù} se:
\marginpar{\small NB: generalmente una tribù è un'algebra se hanno elementi finiti}
\begin{itemize}
    \item $\Omega\in\mathcal{F}$ 
    \item A$\in\mathcal{F}\Rightarrow$ A\SP{C}$\in\mathcal{F}$
    \item per ogni famiglia \underline{numerabile} \{A\SB{i}\}\SPSB{+$\infty$}{i=1}$\subseteq$P($\Omega$),\\ allora 
    $\cup$\SPSB{+$\infty$}{i=1}A\SB{i}$\in\mathcal{F}$
\end{itemize}

$\mathcal{F}$ tribù su $\Omega$. Ogni E$\in\mathcal{F}$ (E è sottoinsieme di $\Omega$) si dice \textbf{Evento}.
I singoletti si chiamano \textbf{eventi elementari}. E si verifica se il risultato dell'esperimento appartiene ad E
$\mathcal{F}$ tribù su $\Omega$ ($\Omega$,$\mathcal{F}$)\\

Dati $\Omega$, $\mathcal{F}$ tribù su $\Omega$ ($\Omega$,$\mathcal{F}$) si chiama \textbf{spazio probabilizzabile}.

($\Omega$,$\mathcal{F}$), una funzione P:$\mathcal{F}\rightarrow\mathcal{R}$ si dice
\textbf{funzione di probabilità} se:
\begin{itemize}
    \item per ogni evento E P(E)$\geq$0
    \item P($\Omega$)=1 
    \item data una famiglia numerabile \{E\SB{i}\}\SPSB{+$\infty$}{i=1} di eventi a 2 a 2 disgiunti:
    \begin{center}
        P($\cup$\SPSB{$\infty$}{i=1}E\SB{i}) = $\sum_{i=1}^{\infty}$P(E\SB{i}) (additività)
    \end{center}
\end{itemize}


\separator

\large{\textbf{Proprietà delle probabilità}}\normalsize
\begin{itemize}
    \item P($\emptyset$) = 0
    \item E$\in\mathcal{F}$ allora P(E\SP{c}) = 1-P(E)
    \item E,F$\in\mathcal{F}$, E$\subseteq$F $\Rightarrow$ P(E)$\leq$P(F)
    \subitem E$\in\mathcal{F}$ P(E)$\leq$1
    \item E,F$\in\mathcal{F}$ P(E$\cup$F)=P(E)+P(F)-P(E$\cap$F)
    \subitem P(E$\cup$F)$\leq$P(E)+P(F)
    \subitem (E\SB{i})\SPSB{n}{i=1}<$\mathcal{F}$, P($\cup$\SPSB{n}{i=1}E\SB{i})=$\sum_{k\in\mathcal{P}(\{1-n\})}$(-1)\SP{\#k+1}
    P($\cap$\SB{j$\in$k}E\SB{j})
    \subitem (E\SB{i})\SPSB{+$\infty$}{i=1}$\subset\mathcal{F}$, P($\cup$\SPSB{$\infty$}{i=1}E\SB{i})$\leq\sum_{i=1}^{\infty}$ 
    P(E\SB{i})
    \item (disuguaglianza di bonferrow) \\
    $\sum_{i=1}^{+\infty}$ P(E\SB{i}) - $\sum_{i<j}$ P(E\SB{i}$\cap$E\SB{j}) $\leq$ P($\cup$\SPSB{+$\infty$}{i=1}E\SB{i}) $\leq$ $\sum_{i=1}^{\infty}$ P(E\SB{i})
\end{itemize}




\section{Probabilità condizionata}

($\Omega$, $\mathcal{F}$, P), E,F$\in\mathcal{F}$ con P(F)$\neq$0, allora la probabilità di E condizionale a F è:

\begin{center}
    P(E$|$F) = $\frac{P(E\cap F)}{P(F)}$
\end{center}


Dato ($\Omega$, $\mathcal{F}$, P) e due sotto tribù $\mathcal{F}\SB{1},\mathcal{F}\SB{2}$ di $\mathcal{F}$
allora $\mathcal{F}\SB{1}e\mathcal{F}\SB{2}$ sono indipendenti se se ogni elemento di $\mathcal{F}\SB{1}$ è indipendente
da ogni elemento di $\mathcal{F}\SB{2}$
\begin{center}
P(E\SB{1}$\cap$E\SB{2} $\vert\mathcal{F}$) = P(E\SB{1}$\vert\mathcal{F}$) $\cdot$ P(E\SB{2}$\vert\mathcal{F}$)    
\end{center}

\clearpage
\section{funzione di probabilità ($\Omega,\mathcal{F},P$)}

\begin{enumerate}
    \item $\Omega$ finito o numerabile
    \subitem $\Omega$ è dato 
    \subitem $\mathcal{F}=\mathcal{P}(\Omega)$
    \subitem P: assegnamo ad ogni singoletto ($\omega\in\Omega$) un probabilità tale che:
    \subsubitem P($\omega$)$\geq$0
    \subsubitem $\sum$P($\omega$)=1
    \subitem A questo punto $\forall$ E $\in\mathcal{F}$ P(E):=$\sum_{\omega\in E}$P($\omega$)
    \item Spazi prodotto
    \subitem considerando più ripetizioni di un esperimento o l'unione di più esperimenti:
    data una famiglia di sottoinsiemi di $\Omega$ dette $\mathcal{A}$. La tribù di $\mathcal{F}$\SB{$\mathcal{A}$}
    generata da $\mathcal{A}$ come la più piccola tribù contenente $\mathcal{A}$
    \begin{center}
        $\mathcal{F}$\SB{$\mathcal{A}$} = $\sigma (\mathcal{A})$ = $\cap \{\mathcal{G} : \mathcal{G}$ è
          tribù in $\Omega$ e $\mathcal{A} \subseteq \mathcal{G}\}$
    \end{center}
    quindi il prodotto $\Omega\SB{1}\times\Omega\SB{2}$, la tribù sarà:
    \begin{center}
        $\mathcal{F}  =  \mathcal{F}\SB{E}\bigotimes\mathcal{F}\SB{E}$ = 
        $\sigma$(\{$E\SB{1}\times E\SB{2}$ : $E\SB{1}\in\mathcal{F}\SB{E}$, $E\SB{2}\in\mathcal{F}\SB{E}$\})


        ($\Omega\SB{1}, \mathcal{F}\SB{1}, P\SB{1}$), ($\Omega\SB{2}, \mathcal{F}\SB{2}, P\SB{2}$)
        \\$\Downarrow$\\
        $\Omega = \Omega\SB{1}\times\Omega\SB{2}$\\
        $\mathcal{F}  =  \mathcal{F}\SB{E}\bigotimes\mathcal{F}\SB{E}$ = 
        $\sigma$(\{$E\SB{1}\times E\SB{2}$ : $E\SB{1}\in\mathcal{F}\SB{E}$, $E\SB{2}\in\mathcal{F}\SB{E}$\})\\
        P : P(E\SB{1}$\times$E\SB{2}) = P\SB{1}(E\SB{1}) $\cdot$ P\SB{2}(E\SB{2})
    \end{center}

    Quindi:\\
    con un numero finito di esperimenti \{($\Omega\SB{i}, \mathcal{F}\SB{i}, P\SB{i}$)\}\SB{i$\in$I}
    allora lo spazio prodotto ha forma:\\
    $\Omega  =  \bigotimes_{i\in I} \Omega_{i}$\\
    $\mathcal{F}  =  \bigotimes_{i\in I} \mathcal{F}_{i}  =  \sigma (\Pi_{i\in I}\ E_{i}: E_{i}\in\mathcal{F}_{i}$ e $\exists$n tc $\forall j\geq n$ E\SB{j}=$\Omega_{i}$)\\
    P = $\bigotimes_{i\in I}$ P\SB{i} cioé P($\Pi_{i\in I}$ E\SB{i}) = $\Pi_{i\in I}$P\SB{i}(E\SB{i})

\end{enumerate}

\newpage
\section{Trasformazioni lineari di variabili aleatorie}
\marginpar{07/04/21}

$X$ variabile aleatoria con legge $F_{X}$. Se X è variabile aleatoria discreta:
\begin{center}
    $\varphi_{Y}(y)=\sum_{x\in g^{-1}(\{y\})}\varphi_{X}(x)$
\end{center}
Se X è variabile aleatoria assolutamente continua abbiamo 2 strategia:
\begin{enumerate}
    \item Ricaviamo la legge di $Y$ usando la forma di $X$ e della funzione $g$
    \item usiamo il teorema generale
\end{enumerate}

\textbf{Teorema del cambio di variabile}

Sia $X$ variabile aleatoria continua di densità $f_{X}$, sia $Y=g(x)$ con $g:\mathbb{R}\rightarrow\mathbb{R}$
continua a tratti t.c. $P(g(x)=0)=0$. Allora:
\begin{center}
    $f_{Y}(y)=\sum_{x\in g^{-1}(\{y\})}\frac{f_{X}(x)}{|g^{1}(x)|} $
\end{center}

\section{Vettori aleatori}
Dato uno spazio probabilizzabile $(\Omega,\mathcal{F},P)$ consideriamo 2 variabili aliatorie $X,Y: $
$\Omega\rightarrow$$\mathbb{R}^{2}$

Def: dati $(\Omega,\mathcal{F},P)$ e $X,Y$ variabili aleatorie su di esso si chiama \textbf{coppia di variabili aleatorie}
o \textbf{variabile aleatoria doppia} o \textbf{2-vettore aleatorio}. La funzione $V:\Omega\rightarrow\mathbb{R}^{2}:
V(\omega)=(X(\omega/Y(\omega)))$. Il supporto del vettore aleatorio $V$:
\begin{center}
    $\mathcal{R}_{V}=\mathcal{R}_{X,Y}=\mathcal{R}_{X}\times\mathcal{R}_{Y}=\{(x,y)\in\mathbb{R}^{2}:x\in\mathcal{R}_{X}
    , y\in\mathcal{R}_{Y} \}$
\end{center}
Def: Data $(X,Y)$ coppia di variabili aleatorie, la sua funzione di ripartizione è:
\begin{center}
    $F_{X,Y}((x,y))=F_{X,Y}(x,y)=P(X\leq x, Y\leq y)$
\end{center}
$F_{X,Y}$ si chiama anche funzione di ripartizione \textbf{congiunta} di $X$ e $Y$

Def: Data $(X,Y)$ coppia di variabili aleatorie, chiameremo \textbf{funzione di ripartizione di $X$ condizionata a $Y$}
la funzione:
\begin{center}
    $F_{X|Y}(x|y):= \frac{F_{X,Y}(x,y)}{F_{Y}(y)}$
\end{center}

Def: Dato $(\Omega,\mathcal{F},P)$ e due tribù $\mathcal{F}_{1}, \mathcal{F}_{2}\subset\mathcal{F},\mathcal{F}_{1},
\mathcal{F}_{2}$ sono indipendenti se lo sono le tribù $\sigma(x)\ e\ \sigma(y)$ da esse generate 

Prop: $X,Y$ sono indipendenti se e solo se:
\begin{center}
    $\forall(x,y)\in\mathbb{R}^{2}\ F_{X|Y}(x|y)=F_{X}(x)F_{Y}(y)$    
\end{center}

Prop: $X,Y$ sono indipendenti se e solo se:
\begin{center}
    $\forall(x,y)\in\mathbb{R}^{2}\ F_{X|Y}(x|y)=F_{X}(x)\ e\ F_{Y|X}(y|x)=F_{Y}(y)$    
\end{center}

\section{Vettori aleatori discreti}

Def: Siano $X,Y$ variabili aleatorie discrete su $(\Omega,\mathcal{F},P)$ chiamiamo \textbf{densità discrete congiunte}
la funzione $\varphi_{X,Y}:\mathbb{R}^{2}\rightarrow [0,1]$ definita:
\begin{center}
        $\varphi_{X,Y}(x,y)=P(X=x, Y=y)$
\end{center}
la \textbf{densità discreta di $X$ condizionata a $Y$} è  $\varphi_{X,Y}$ definità:

\newpage
\thispagestyle{fancy}
\chead{ottava settimana}
\section{Schema o processo di Bernoulli}
\marginpar{21/04/21}

Dati infiniti esperimenti indipendenti e identicamente distribuiti
\begin{center}
    $(X_{i})_{i}\in\mathbb{N}\ iid\ X_{i}\sim bin(1,p)$
\end{center}
$\Omega=\{0,1\}^{\mathbb{N}/\{0\}}$\\
Tribù $\mathcal{F}$ generata dai cilindri\\
P uguale al prodotto delle probabilità delle componenti
\subsection{Cilindri}
I cilindri sono sottoinsiemi $c\subseteq\Omega$ tali che esiste un $n\in\mathbb{N}/\{0\}$ e un vettore $v\in\{0,1\}^{n}$:
\begin{center}
    $C=\{\omega\in\Omega :\omega_{i}=v_{i}\ 1\leq i\leq n\}$
\end{center}
Es:
\begin{itemize}
    \item un successo seguito da due insuccessi:
    \subitem Cilindro: $n=3\ v=(1,0,0)\Rightarrow prob=p(1-p)^{2}$
    \item primo successo al k-esimo lancio:
    \subitem Cilindro: $(0,0,...,0_{k-1},1_{k})\Rightarrow prob=(1-p)^{k-1}p$
    \item prob 3° lancio sia un successo:
    \subitem $(\cdot\cdot\cdot 1*)=(001)\cup(101)\cup(011)\cup(111)$
    \subitem $P(\cdot\cdot\cdot 1*)=\sum P(...)=(1-p)^{2}p+2(1-p)p^{2}+p^{3}=P(p+(1-p))^{2}$ 
\end{itemize}

\section{Geometriche}
Una varibile aleatoria ($T_{1}:=inf\{i\geq 1: \omega_{i}=1\}$) è una geometrica di parametro p $X\sim geom(p)$
se è l'istante precedente al primo successo in un processo di Bernoulli di parametro p

cdf di una geometria:
\begin{equation}
    F_{X}(x)
  \begin{cases}
    0\ \ x<0\\
    \sum_{k=0}^{x} \varphi_{x}(k)=1-(1-p)^{x}\ \ x \geq 0
  \end{cases}
\end{equation}

\textbf{Assenza di memoria:} $\forall n,k\in\mathbb{N}\ \ \ P(x\geq n+k|X\geq n)=P(X\geq k)$\\
es:
\begin{center}
    $(Y\geq 60+30|Y\geq 60)=(Y\geq 30)=(1-p)^{30}$
\end{center}

\section{Binominali negative}
$T_{n}$= istante dell'n-esimo successo

$T_{1}:=inf\{i\geq 1: \omega_{i}=1\}$\\
$T_{n+1}:=inf\{i\geq T_{n}: \omega_{i}=1\}\ n\geq 1$

$X$ è una variabile aleatoria binominale negativa (o di pascal) di parametri n e p se è il numero di insuccessi precedenti
all'n-ennesimo successo di uno schema di bernoulli di parametro p $X\sim NB(n,p)$

\begin{equation}
    pn k\in\mathbb{N} \varphi_{x}(k)
    \begin{cases}
        = P(x=k)=P(T_{n}=k+n)\\
        = P(\omega_{n+k}=1,\sum_{j=1}\omega_{j}=n-1)\\
        = p \binom{k+n-1}{n-1} p^{n-1}(1-p)^{k} 
    \end{cases}    
    \Rightarrow \binom{k+n-1}{n-1} p^{n}(1-p)^{k} 
\end{equation}

\newpage
\section{Riproducibilità}\marginpar{22/04/21}
Una famiglia di variabili aleatorie si dice riproducibile se sommando 2 variabili aleatorie indipendenti appartenenti
a quella famiglia abbiano ancora una variabile aleatoria della medesima famiglia

\textbf{Prop:} La famiglia delle binominali a parametro $p$ fissato è riproducibile. Se $X\sim bin(n,p),\ 
Y\sim bin(m,p)$, $X$ e $Y$ indipendenti allora:
\begin{center}
    $X+Y\sim bin(n+m,p)$
\end{center}

\section{Ipergeometriche}
Data un urna con $n$ biglie bianche e $n$ biglie nere, contiamo le bianche:\\
- con reimmissione abbiamo $bin(k,\frac{n}{m+n})$\\
- senza reimmissione usiamo un'ipergeometrica

\textbf{Def:} Si chiama ipergeometrica di parametri $k,n,m$ la variabile aleatoria che conta il numero di bianche
tra le estratte senza reimmissione
\begin{center}
    $X\sim hyp(k,n,m)$
\end{center}

$\varphi_{x}(b):
\begin{cases}
    \frac{\binom{m}{b}\binom{n}{k-b}}{\binom{n+m}{k}}\\
    0\ \ altrimenti
\end{cases}$

\newpage
\thispagestyle{fancy}
\chead{Nona settimana}
\marginpar{28/04/2021}

\textbf{Prop:} Siano $\{a_{i}\}_{i\in\mathbb{N}},\ \{b_{i}\}_{i\in\mathbb{N}}$ interi non negativi che tendono
in modo monotono a $+\infty$, $\lim_{i \to \infty} a_{i}=\lim_{i\to\infty} b_{i}=+\infty$ o tali che \\
$\lim_{i\to\infty} \frac{a_{i}}{b_{i}+a_{i}}= \alpha,\ \alpha\in[0,1]$, allora:
\begin{center}
    $\frac{\binom{a_{i}}{k}\binom{b_{i}}{n-k}}{\binom{a_{i}+b_{i}}{n}} \rightarrow_{i\to\infty}
    \binom{n}{k}\alpha^{k}(1-\alpha)^{n-k}$
\end{center}

\section{Poisson}
\textbf{Def:} $X$ è variabile aleatoria di Poisson di parametro $\lambda >0$ se:
\begin{center}
    $\varphi_{x}(k)\begin{cases}
        \frac{\lambda^{k}}{k!}\cdot e^{-\lambda}\ \ k\in\mathbb{N}\\
        0\ \ altrimenti
    \end{cases}$
\end{center} 
e si denota come $X\sim Pois(\lambda)$

Es:\\
in una partita di calcio vengono segnati 2.5 gol di media. X determina la probabilità di fare gol in un intervallo:\\
$\Rightarrow$ dividiamo 90' in 5 intervalli: $X\sim bin(5,1/2)$\\
$\Rightarrow$ dividiamo in 20 intervalli: $X\sim bin(20,1/8)$\\
$\Rightarrow$ dividiamo in 90 intervalli: $X\sim bin(90,1/36)$

Questa successione tende a una variabile aleatoria di Poisson

Oss: Poisson viene a volte utilizzato come descrizione di una binomiale con $n,p$ piccoli o grandi, non precisi

\textbf{Prop:} $\{p_{n}\}_{n}$ successione di numeri in $[0,1]$ tale che $\lim_{x\to\infty}n\cdot p_{n}=
\lambda\in\mathbb{R}^{+}$ allora $\forall k\in\mathbb{N}$:
\begin{center}
    $\lim_{n\to\infty} \binom{n}{k}p_{n}^{k}(1-p_{n})^{n-k}=\frac{\lambda^{k}}{k!}e^{-\lambda}$
\end{center}

\newpage
\textbf{Prop:} Le variabili aleatorie di Poisson sono riproducibili. 
$X\sim Pois(\lambda_{1}),\ Y\sim Pois(\lambda_{2})$:
\begin{center}
    $X+Y\sim Pois(\lambda_{1}+\lambda_{2})$
\end{center}
\end{document}