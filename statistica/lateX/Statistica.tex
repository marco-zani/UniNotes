\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}

\usepackage{geometry}

\usepackage{multicol}
\usepackage{listings}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\geometry{margin=0.6in}


\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\title{Statistica}


\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\def\SPSB#1#2{\rlap{\textsuperscript{#1}}\SB{#2}}
\def\SP#1{\textsuperscript{#1}}
\def\SB#1{\textsubscript{#1}}

\def\separator{\begin{center}    \rule{100pt}{0.5pt}\end{center}}

\begin{document}

\section{Introduzione}

probabilità $\rightarrow$ misurare l'incertezza\\
statistica:
\begin{itemize}
    \item descrittiva
    \item differenziale $\rightarrow$  \underline{campione casuale} per \underline{stimare} un esito
\end{itemize}

\separator

probabilità:  
\begin{center}
    $\frac{casi favorevoli}{casi totali}$
\textbf{\underline{SE}} equiprobabili
\end{center}

per contare i casi ci si appoggia alla \underline{combinatoria}

\separator

partizione: separazione di A in sottoinsiemi senza elementi comuni

\separator

\textbf{NB:} 
\begin{itemize}
    \item $\wedge$ - and $\rightarrow$ A$\cap$B = \{x$|$x$\in$A$\wedge$x$\in$B\}
    \item $\vee$ - or $\rightarrow$ A$\cup$B = \{x$|$x$\in$A$\vee$x$\in$B\}
\end{itemize} 


\textbf{Principi della combinatoria:}
\begin{enumerate}
    \item A insieme, \{E\textsubscript{i}\}\SPSB{n}{i=1} partizione di A $\rightarrow$ \#A = $\sum_{i = 1}^{n}$ 
    \#E\textsubscript{i} 
    \begin{itemize}
        \item A,B insiemi, AxB è l'insieme di coppie ordinate (a,b)
    \end{itemize}  
    \item \#(AxB) = \#A$\cdot$\#B $\rightarrow$ \{A\textsubscript{i}\}\SPSB{n}{i = 1} = $\bigotimes$\SPSB{n}{i=1} A\SB{i}
    \item A,B, \#(A$\cup$B) = \underline{\#A + \#B - \#(A{$\cap$}B)} (non perfetto) $\rightarrow$
    \begin{center}
        \item \# $\cup$\SPSB{n}{i= 1} A\SB{i} = $\sum_{i = 1}^{n}$ \#A\SB{i} - $\sum_{i<j}^{}$\#(A\SB{i}$\cap$A\SB{j}) +
        $\sum_{i<j<k}^{}$\#(A\SB{i}$\cap$A\SB{j}$\cap$A\SB{k}) + \dots
        
        $\downarrow$

        \textbf{+(-1)\SP{n+1} \#$\cap$\SPSB{n}{i=1}A\SB{i}}
    \end{center}
\end{enumerate}




\section{Permutazioni e anagrammi}

Fattoriale $\rightarrow$ x! = 9! = 9$\cdot$8$\cdot$7$\cdot$6$\cdot\dots\cdot$2$\cdot$1

\textbf{NB:} 0! = 1

\begin{itemize}
    \item "prendiamo" ha 9! anagrammi
    \item "anagramma" ha tre ripetizioni di a e due ripetizioni di m, quindi per calcolare i casi unici:   
\end{itemize}
\begin{center}
    \Large $\frac{9!}{3!{\cdot}2!}$ \normalsize

    $\downarrow$
 \end{center} 
per calcolare la probabilitàdegli elementi n, ma mi interessano solo k elementi allora:
\begin{center}
    \Large $\frac{n!}{(n-k)!}$ \normalsize
\end{center}
se non sono interessato all'ordine, allora:
\begin{center}
    \Large $\frac{n!}{(n-k)!k!}$ $\Rightarrow$ $\binom{n}{k}$ \normalsize
\end{center}
chiamato anche \textbf{coefficente binominiale}

\separator

\textbf{Proprietà:}
\begin{itemize}
    \item $\binom{n}{k}$ = $\binom{n}{n-k}$
    \item $\binom{n}{0}$ = $\binom{n}{n}$ = 1
    \item $\sum_{k=0}^{n}$ $\binom{n}{k}$  = 2\SP{n}
    \item $\binom{n}{k}$ + $\binom{n}{k+1}$ = $\binom{n+1}{k+1}$  
\end{itemize}




\section{Esperimenti aliatori}

Un esperimento si definisce \textbf{aliatorio} o casuale se con i dati iniziali il risultato è incerto.
I risultati a 2 a2 incompatibili di un esperimento aliatorio sono chiamati \textbf{esiti}.
$\Omega$ denota lo \textbf{spazio degli esiti}.
Un \textbf{evento} è un osservabile di un esperimento aliatorio.

Una parte di $\Omega$ può essere considerata come famiglia:
\begin{center}
    $\mathcal{F}$ $\subseteq$ P($\Omega$)
\end{center}
Questa è definita come \textbf{algebra} se:
\begin{itemize}
    \item $\Omega\in\mathcal{F}$
    \item se A$\in\mathcal{F}$ allora A\SP{c}$\in\mathcal{F}$
    \item se A,B$\in\mathcal{F}$, allora A$\cup$B$\in\mathcal{F}$
    \subitem potremmo scrivere anche \{A\SB{i}\}\SPSB{n}{i=1}$\subseteq\mathcal{F}$ allora 
    $\cup$\SPSB{n}{i=1}A\SB{i}$\in\mathcal{F}$
\end{itemize}

\separator

\large{\textbf{Proprietà}}\normalsize
\begin{itemize}
    \item $\emptyset\in\mathcal{F}$
    \item se A,B$\in\mathcal{F}$ allora A$\cap$B$\in\mathcal{F}$
    \item se \{A\SB{i}\}\SPSB{n}{i=1}$\subseteq\mathcal{F}$ allora $\cap$\SPSB{n}{i=1}A\SB{i}$\in\mathcal{F}$
    \item se A,B $\in\mathcal{F}$, allora A$\cdot$B $\in\mathcal{F}$
    \item se A,B $\in\mathcal{F}$, allora A$\triangle$B $\in\mathcal{F}$
\end{itemize}

\separator
 
$\mathcal{F}\subseteq$P($\Omega)$ è una \textbf{tribù} se:
\marginpar{\small NB: generalmente una tribù è un'algebra se hanno elementi finiti}
\begin{itemize}
    \item $\Omega\in\mathcal{F}$ 
    \item A$\in\mathcal{F}\Rightarrow$ A\SP{C}$\in\mathcal{F}$
    \item per ogni famiglia \underline{numerabile} \{A\SB{i}\}\SPSB{+$\infty$}{i=1}$\subseteq$P($\Omega$),\\ allora 
    $\cup$\SPSB{+$\infty$}{i=1}A\SB{i}$\in\mathcal{F}$
\end{itemize}

$\mathcal{F}$ tribù su $\Omega$. Ogni E$\in\mathcal{F}$ (E è sottoinsieme di $\Omega$) si dice \textbf{Evento}.
I singoletti si chiamano \textbf{eventi elementari}. E si verifica se il risultato dell'esperimento appartiene ad E
$\mathcal{F}$ tribù su $\Omega$ ($\Omega$,$\mathcal{F}$)\\

Dati $\Omega$, $\mathcal{F}$ tribù su $\Omega$ ($\Omega$,$\mathcal{F}$) si chiama \textbf{spazio probabilizzabile}.

($\Omega$,$\mathcal{F}$), una funzione P:$\mathcal{F}\rightarrow\mathcal{R}$ si dice
\textbf{funzione di probabilità} se:
\begin{itemize}
    \item per ogni evento E P(E)$\geq$0
    \item P($\Omega$)=1 
    \item data una famiglia numerabile \{E\SB{i}\}\SPSB{+$\infty$}{i=1} di eventi a 2 a 2 disgiunti:
    \begin{center}
        P($\cup$\SPSB{$\infty$}{i=1}E\SB{i}) = $\sum_{i=1}^{\infty}$P(E\SB{i}) (additività)
    \end{center}
\end{itemize}


\separator

\large{\textbf{Proprietà delle probabilità}}\normalsize
\begin{itemize}
    \item P($\emptyset$) = 0
    \item E$\in\mathcal{F}$ allora P(E\SP{c}) = 1-P(E)
    \item E,F$\in\mathcal{F}$, E$\subseteq$F $\Rightarrow$ P(E)$\leq$P(F)
    \subitem E$\in\mathcal{F}$ P(E)$\leq$1
    \item E,F$\in\mathcal{F}$ P(E$\cup$F)=P(E)+P(F)-P(E$\cap$F)
    \subitem P(E$\cup$F)$\leq$P(E)+P(F)
    \subitem (E\SB{i})\SPSB{n}{i=1}<$\mathcal{F}$, P($\cup$\SPSB{n}{i=1}E\SB{i})=$\sum_{k\in\mathcal{P}(\{1-n\})}$(-1)\SP{\#k+1}
    P($\cap$\SB{j$\in$k}E\SB{j})
    \subitem (E\SB{i})\SPSB{+$\infty$}{i=1}$\subset\mathcal{F}$, P($\cup$\SPSB{$\infty$}{i=1}E\SB{i})$\leq\sum_{i=1}^{\infty}$ 
    P(E\SB{i})
    \item (disuguaglianza di bonferrow) \\
    $\sum_{i=1}^{+\infty}$ P(E\SB{i}) - $\sum_{i<j}$ P(E\SB{i}$\cap$E\SB{j}) $\leq$ P($\cup$\SPSB{+$\infty$}{i=1}E\SB{i}) $\leq$ $\sum_{i=1}^{\infty}$ P(E\SB{i})
\end{itemize}




\section{Probabilità condizionata}

($\Omega$, $\mathcal{F}$, P), E,F$\in\mathcal{F}$ con P(F)$\neq$0, allora la probabilità di E condizionale a F è:

\begin{center}
    P(E$|$F) = $\frac{P(E\cap F)}{P(F)}$
\end{center}


Dato ($\Omega$, $\mathcal{F}$, P) e due sotto tribù $\mathcal{F}\SB{1},\mathcal{F}\SB{2}$ di $\mathcal{F}$
allora $\mathcal{F}\SB{1}e\mathcal{F}\SB{2}$ sono indipendenti se se ogni elemento di $\mathcal{F}\SB{1}$ è indipendente
da ogni elemento di $\mathcal{F}\SB{2}$
\begin{center}
P(E\SB{1}$\cap$E\SB{2} $\vert\mathcal{F}$) = P(E\SB{1}$\vert\mathcal{F}$) $\cdot$ P(E\SB{2}$\vert\mathcal{F}$)    
\end{center}

\clearpage
\section{funzione di probabilità ($\Omega,\mathcal{F},P$)}

\begin{enumerate}
    \item $\Omega$ finito o numerabile
    \subitem $\Omega$ è dato 
    \subitem $\mathcal{F}=\mathcal{P}(\Omega)$
    \subitem P: assegnamo ad ogni singoletto ($\omega\in\Omega$) un probabilità tale che:
    \subsubitem P($\omega$)$\geq$0
    \subsubitem $\sum$P($\omega$)=1
    \subitem A questo punto $\forall$ E $\in\mathcal{F}$ P(E):=$\sum_{\omega\in E}$P($\omega$)
    \item Spazi prodotto
    \subitem considerando più ripetizioni di un esperimento o l'unione di più esperimenti:
    data una famiglia di sottoinsiemi di $\Omega$ dette $\mathcal{A}$. La tribù di $\mathcal{F}$\SB{$\mathcal{A}$}
    generata da $\mathcal{A}$ come la più piccola tribù contenente $\mathcal{A}$
    \begin{center}
        $\mathcal{F}$\SB{$\mathcal{A}$} = $\sigma (\mathcal{A})$ = $\cap \{\mathcal{G} : \mathcal{G}$ è
          tribù in $\Omega$ e $\mathcal{A} \subseteq \mathcal{G}\}$
    \end{center}
    quindi il prodotto $\Omega\SB{1}\times\Omega\SB{2}$, la tribù sarà:
    \begin{center}
        $\mathcal{F}  =  \mathcal{F}\SB{E}\bigotimes\mathcal{F}\SB{E}$ = 
        $\sigma$(\{$E\SB{1}\times E\SB{2}$ : $E\SB{1}\in\mathcal{F}\SB{E}$, $E\SB{2}\in\mathcal{F}\SB{E}$\})


        ($\Omega\SB{1}, \mathcal{F}\SB{1}, P\SB{1}$), ($\Omega\SB{2}, \mathcal{F}\SB{2}, P\SB{2}$)
        \\$\Downarrow$\\
        $\Omega = \Omega\SB{1}\times\Omega\SB{2}$\\
        $\mathcal{F}  =  \mathcal{F}\SB{E}\bigotimes\mathcal{F}\SB{E}$ = 
        $\sigma$(\{$E\SB{1}\times E\SB{2}$ : $E\SB{1}\in\mathcal{F}\SB{E}$, $E\SB{2}\in\mathcal{F}\SB{E}$\})\\
        P : P(E\SB{1}$\times$E\SB{2}) = P\SB{1}(E\SB{1}) $\cdot$ P\SB{2}(E\SB{2})
    \end{center}

    Quindi:\\
    con un numero finito di esperimenti \{($\Omega\SB{i}, \mathcal{F}\SB{i}, P\SB{i}$)\}\SB{i$\in$I}
    allora lo spazio prodotto ha forma:\\
    $\Omega  =  \bigotimes_{i\in I} \Omega_{i}$\\
    $\mathcal{F}  =  \bigotimes_{i\in I} \mathcal{F}_{i}  =  \sigma (\Pi_{i\in I}\ E_{i}: E_{i}\in\mathcal{F}_{i}$ e $\exists$n tc $\forall j\geq n$ E\SB{j}=$\Omega_{i}$)\\
    P = $\bigotimes_{i\in I}$ P\SB{i} cioé P($\Pi_{i\in I}$ E\SB{i}) = $\Pi_{i\in I}$P\SB{i}(E\SB{i})

\end{enumerate}

\newpage
\section{Trasformazioni lineari di variabili aleatorie}
\marginpar{07/04/21}

$X$ variabile aleatoria con legge $F_{X}$. Se X è variabile aleatoria discreta:
\begin{center}
    $\varphi_{Y}(y)=\sum_{x\in g^{-1}(\{y\})}\varphi_{X}(x)$
\end{center}
Se X è variabile aleatoria assolutamente continua abbiamo 2 strategia:
\begin{enumerate}
    \item Ricaviamo la legge di $Y$ usando la forma di $X$ e della funzione $g$
    \item usiamo il teorema generale
\end{enumerate}

\textbf{Teorema del cambio di variabile}

Sia $X$ variabile aleatoria continua di densità $f_{X}$, sia $Y=g(x)$ con $g:\mathbb{R}\rightarrow\mathbb{R}$
continua a tratti t.c. $P(g(x)=0)=0$. Allora:
\begin{center}
    $f_{Y}(y)=\sum_{x\in g^{-1}(\{y\})}\frac{f_{X}(x)}{|g^{1}(x)|} $
\end{center}

\section{Vettori aleatori}
Dato uno spazio probabilizzabile $(\Omega,\mathcal{F},P)$ consideriamo 2 variabili aliatorie $X,Y: $
$\Omega\rightarrow$$\mathbb{R}^{2}$

Def: dati $(\Omega,\mathcal{F},P)$ e $X,Y$ variabili aleatorie su di esso si chiama \textbf{coppia di variabili aleatorie}
o \textbf{variabile aleatoria doppia} o \textbf{2-vettore aleatorio}. La funzione $V:\Omega\rightarrow\mathbb{R}^{2}:
V(\omega)=(X(\omega/Y(\omega)))$. Il supporto del vettore aleatorio $V$:
\begin{center}
    $\mathcal{R}_{V}=\mathcal{R}_{X,Y}=\mathcal{R}_{X}\times\mathcal{R}_{Y}=\{(x,y)\in\mathbb{R}^{2}:x\in\mathcal{R}_{X}
    , y\in\mathcal{R}_{Y} \}$
\end{center}
Def: Data $(X,Y)$ coppia di variabili aleatorie, la sua funzione di ripartizione è:
\begin{center}
    $F_{X,Y}((x,y))=F_{X,Y}(x,y)=P(X\leq x, Y\leq y)$
\end{center}
$F_{X,Y}$ si chiama anche funzione di ripartizione \textbf{congiunta} di $X$ e $Y$

Def: Data $(X,Y)$ coppia di variabili aleatorie, chiameremo \textbf{funzione di ripartizione di $X$ condizionata a $Y$}
la funzione:
\begin{center}
    $F_{X|Y}(x|y):= \frac{F_{X,Y}(x,y)}{F_{Y}(y)}$
\end{center}

Def: Dato $(\Omega,\mathcal{F},P)$ e due tribù $\mathcal{F}_{1}, \mathcal{F}_{2}\subset\mathcal{F},\mathcal{F}_{1},
\mathcal{F}_{2}$ sono indipendenti se lo sono le tribù $\sigma(x)\ e\ \sigma(y)$ da esse generate 

Prop: $X,Y$ sono indipendenti se e solo se:
\begin{center}
    $\forall(x,y)\in\mathbb{R}^{2}\ F_{X|Y}(x|y)=F_{X}(x)F_{Y}(y)$    
\end{center}

Prop: $X,Y$ sono indipendenti se e solo se:
\begin{center}
    $\forall(x,y)\in\mathbb{R}^{2}\ F_{X|Y}(x|y)=F_{X}(x)\ e\ F_{Y|X}(y|x)=F_{Y}(y)$    
\end{center}

\section{Vettori aleatori discreti}

Def: Siano $X,Y$ variabili aleatorie discrete su $(\Omega,\mathcal{F},P)$ chiamiamo \textbf{densità discrete congiunte}
la funzione $\varphi_{X,Y}:\mathbb{R}^{2}\rightarrow [0,1]$ definita:
\begin{center}
        $\varphi_{X,Y}(x,y)=P(X=x, Y=y)$
\end{center}
la \textbf{densità discreta di $X$ condizionata a $Y$} è  $\varphi_{X,Y}$ definità:

\newpage
\section{Vettori aleatori misti}
caso speciale di va discreta e va continua

\begin{center}
    \textbf{Modelli di variabili aleatorie discrete}
\end{center}

\section{Bernoulliane}
\textbf{Def:}\\ Una variabile aleatoria di dice bernoulliana di parametro $p\in[0,1]$ se la densità discreta è:
\begin{center}
    $X\sim bin(1,p)$

    $\varphi_{x}(x)=\begin{cases}            
        p & x=1\\
        1-p & x=0\\
        0 & altrimenti
        
    \end{cases}$
\end{center}
la cdf di una bernoulliana ($p$) è 
$F_{x}(x)=
\begin{cases}
        0 & x < 0\\
        1-p & 0 \leq x<1\\
        1 & x\geq 1
\end{cases}$

\section{Binominale}
\textbf{Def:}\\ una variabile aleatoria discreta $X$ è binominale di parametri $n$ e $p$ se è la somma di $n$ bernulliane($p$) 
indipendenti
\begin{center}
    $X\sim bin(n,p)$

    $\varphi_{x}(k)=
    \begin{cases}
            \binom{n}{k}p^{k}(1-p)^{n-k} & k\in\{0,...,n\}\\
            0 & altrimenti
        
    \end{cases}$
\end{center}

\newpage
\section{Schema o processo di Bernoulli}
\marginpar{21/04/21}

Dati infiniti esperimenti indipendenti e identicamente distribuiti
\begin{center}
    $(X_{i})_{i}\in\mathbb{N}\ iid\ X_{i}\sim bin(1,p)$
\end{center}
$\Omega=\{0,1\}^{\mathbb{N}/\{0\}}$\\
Tribù $\mathcal{F}$ generata dai cilindri\\
P uguale al prodotto delle probabilità delle componenti
\subsection{Cilindri}
I cilindri sono sottoinsiemi $c\subseteq\Omega$ tali che esiste un $n\in\mathbb{N}/\{0\}$ e un vettore $v\in\{0,1\}^{n}$:
\begin{center}
    $C=\{\omega\in\Omega :\omega_{i}=v_{i}\ 1\leq i\leq n\}$
\end{center}
Es:
\begin{itemize}
    \item un successo seguito da due insuccessi:
    \subitem Cilindro: $n=3\ v=(1,0,0)\Rightarrow prob=p(1-p)^{2}$
    \item primo successo al k-esimo lancio:
    \subitem Cilindro: $(0,0,...,0_{k-1},1_{k})\Rightarrow prob=(1-p)^{k-1}p$
    \item prob 3° lancio sia un successo:
    \subitem $(\cdot\cdot\cdot 1*)=(001)\cup(101)\cup(011)\cup(111)$
    \subitem $P(\cdot\cdot\cdot 1*)=\sum P(...)=(1-p)^{2}p+2(1-p)p^{2}+p^{3}=P(p+(1-p))^{2}$ 
\end{itemize}

\section{Geometriche}
Una varibile aleatoria ($T_{1}:=inf\{i\geq 1: \omega_{i}=1\}$) è una geometrica di parametro p $X\sim geom(p)$
se è l'istante precedente al primo successo in un processo di Bernoulli di parametro p

cdf di una geometria:
\begin{equation}
    F_{X}(x)
  \begin{cases}
    0\ \ x<0\\
    \sum_{k=0}^{x} \varphi_{x}(k)=1-(1-p)^{x}\ \ x \geq 0
  \end{cases}
\end{equation}

\textbf{Assenza di memoria:} $\forall n,k\in\mathbb{N}\ \ \ P(x\geq n+k|X\geq n)=P(X\geq k)$\\
es:
\begin{center}
    $(Y\geq 60+30|Y\geq 60)=(Y\geq 30)=(1-p)^{30}$
\end{center}

\section{Binominali negative}
$T_{n}$= istante dell'n-esimo successo

$T_{1}:=inf\{i\geq 1: \omega_{i}=1\}$\\
$T_{n+1}:=inf\{i\geq T_{n}: \omega_{i}=1\}\ n\geq 1$

$X$ è una variabile aleatoria binominale negativa (o di pascal) di parametri n e p se è il numero di insuccessi precedenti
all'n-ennesimo successo di uno schema di bernoulli di parametro p $X\sim NB(n,p)$

\begin{equation}
    pn k\in\mathbb{N} \varphi_{x}(k)
    \begin{cases}
        = P(x=k)=P(T_{n}=k+n)\\
        = P(\omega_{n+k}=1,\sum_{j=1}\omega_{j}=n-1)\\
        = p \binom{k+n-1}{n-1} p^{n-1}(1-p)^{k} 
    \end{cases}    
    \Rightarrow \binom{k+n-1}{n-1} p^{n}(1-p)^{k} 
\end{equation}

\newpage
\section{Riproducibilità}\marginpar{22/04/21}
Una famiglia di variabili aleatorie si dice riproducibile se sommando 2 variabili aleatorie indipendenti appartenenti
a quella famiglia abbiano ancora una variabile aleatoria della medesima famiglia

\textbf{Prop:} La famiglia delle binominali a parametro $p$ fissato è riproducibile. Se $X\sim bin(n,p),\ 
Y\sim bin(m,p)$, $X$ e $Y$ indipendenti allora:
\begin{center}
    $X+Y\sim bin(n+m,p)$
\end{center}

\section{Ipergeometriche}
Data un urna con $n$ biglie bianche e $n$ biglie nere, contiamo le bianche:\\
- con reimmissione abbiamo $bin(k,\frac{n}{m+n})$\\
- senza reimmissione usiamo un'ipergeometrica

\textbf{Def:} Si chiama ipergeometrica di parametri $k,n,m$ la variabile aleatoria che conta il numero di bianche
tra le estratte senza reimmissione
\begin{center}
    $X\sim hyp(k,n,m)$
\end{center}

$\varphi_{x}(b):
\begin{cases}
    \frac{\binom{m}{b}\binom{n}{k-b}}{\binom{n+m}{k}}\\
    0\ \ altrimenti
\end{cases}$

\newpage
\marginpar{28/04/2021}

\textbf{Prop:} Siano $\{a_{i}\}_{i\in\mathbb{N}},\ \{b_{i}\}_{i\in\mathbb{N}}$ interi non negativi che tendono
in modo monotono a $+\infty$, $\lim_{i \to \infty} a_{i}=\lim_{i\to\infty} b_{i}=+\infty$ o tali che \\
$\lim_{i\to\infty} \frac{a_{i}}{b_{i}+a_{i}}= \alpha,\ \alpha\in[0,1]$, allora:
\begin{center}
    $\frac{\binom{a_{i}}{k}\binom{b_{i}}{n-k}}{\binom{a_{i}+b_{i}}{n}} \rightarrow_{i\to\infty}
    \binom{n}{k}\alpha^{k}(1-\alpha)^{n-k}$
\end{center}

\section{Poisson}
\textbf{Def:} $X$ è variabile aleatoria di Poisson di parametro $\lambda >0$ se:
\begin{center}
    $\varphi_{x}(k)\begin{cases}
        \frac{\lambda^{k}}{k!}\cdot e^{-\lambda}\ \ k\in\mathbb{N}\\
        0\ \ altrimenti
    \end{cases}$
\end{center} 
e si denota come $X\sim Pois(\lambda)$

Es:\\
in una partita di calcio vengono segnati 2.5 gol di media. X determina la probabilità di fare gol in un intervallo:\\
$\Rightarrow$ dividiamo 90' in 5 intervalli: $X\sim bin(5,1/2)$\\
$\Rightarrow$ dividiamo in 20 intervalli: $X\sim bin(20,1/8)$\\
$\Rightarrow$ dividiamo in 90 intervalli: $X\sim bin(90,1/36)$

Questa successione tende a una variabile aleatoria di Poisson

Oss: Poisson viene a volte utilizzato come descrizione di una binomiale con $n,p$ piccoli o grandi, non precisi

\textbf{Prop:} $\{p_{n}\}_{n}$ successione di numeri in $[0,1]$ tale che $\lim_{x\to\infty}n\cdot p_{n}=
\lambda\in\mathbb{R}^{+}$ allora $\forall k\in\mathbb{N}$:
\begin{center}
    $\lim_{n\to\infty} \binom{n}{k}p_{n}^{k}(1-p_{n})^{n-k}=\frac{\lambda^{k}}{k!}e^{-\lambda}$
\end{center}

\newpage
\textbf{Prop:} Le variabili aleatorie di Poisson sono riproducibili. 
$X\sim Pois(\lambda_{1}),\ Y\sim Pois(\lambda_{2})$:
\begin{center}
    $X+Y\sim Pois(\lambda_{1}+\lambda_{2})$
\end{center}

\section{Speranza/Valore atteso/Media}
\begin{center}
    \textbf{Caso variabile aleatoria discreta}
\end{center}

\textbf{Def:}\\La speranza di una variabile aleatoria discreta è il baricentro della sua distribuzione
\begin{center}
    $E[X]=\sum_{x\in\mathcal{R}_{x}}\ x\cdot\varphi_{x}(x)$
\end{center}

Oss: se prendo un esito $Y=y$ nella mia tribù e considerò $P(.|Y)$
\begin{center}
    $E[X|Y]=\sum_{x\in\mathcal{R}_{x}}\ x\cdot P(X=x|Y=y)=\sum x\cdot\varphi_{x|y}(X|Y)$
\end{center}

\textbf{Teorema}\\Sia $X$ una variabile aleatoria discreta con densità discreta $\varphi_{x}$ e sia $Y=g(x)$, allora:
\begin{center}
    $E[Y]=\sum_{k\in\mathcal{R}_{x}}\ g(k)\cdot\varphi_{x}(x)$
\end{center}

\textbf{Teorema}\\Sia ($X,Y$) un vettore aleatorio discreto con densità congiunta $\varphi_{x,y}$ e sia $Z=g(x,y)$,
$g:\mathbb{R}^{2}\rightarrow\mathbb{R}$, allora:
\begin{center}
    $E[Z]=\sum_{j\in\mathcal{R}_{y}}\sum_{k\in\mathcal{R}_{x}}\ g(k,j)\cdot\varphi_{xy}(k,j)$
\end{center}

\newpage
\textbf{Prop}\marginpar{29/04/2021}\\il valore atteso possiede le seguenti proprietà:
\begin{itemize}
    \item Linearità: Siano $X,Y$ variabili aleatorie discrete e $a,b\in\mathbb{R}$, allora:
    \begin{center}
        $E[aX+bY]=aE[X]+bE[Y]$
    \end{center}
    \item Prodotto di variabili aleatorie indipendenti: Siano $X,Y$ variabili aleatorie discrete e indipendenti, allora:
    \begin{center}
        $E[XY]=E[X]\cdot E[Y]$
    \end{center}
    \item Monotonia: $X$ variabile aleatoria discreta, se $X\geq 0$ allora $E[X]\geq 0$. L'uguaglianza vale solo se $X\equiv 0$
\end{itemize}

Corollario: Se $X,Y$ variabili aleatorie discrete tali che $P((X\geq Y)=1)$ allora $E[X]\geq E[Y]$. In più se $E[X]=E[Y]$ Allora
$X=Y$

\textbf{Speranza di variabili aleatorie discrete note}

\begin{tabular}{l c c l}
    Bernoulliane & $X\sim bin(1,p)$ & $\Rightarrow$ & $E[X]=p$\\
    Binomiali & $X\sim bin(n,p)$ & $\Rightarrow$ & $E[X]=n\cdot p$\\
    Poisson & $X\sim pois(\lambda)$ & $\Rightarrow$ & $E[X]=\lambda$\\
    Ipergeometriche & $X\sim hyp(k,m,n)$ & $\Rightarrow$ & $E[X]=k\cdot\frac{m}{n+m}$\\
    Geometriche & $X\sim geom(p)$ & $\Rightarrow$ & $E[X]=\frac{1-p}{p}$\\
    Binomiali Negative & $X\sim NB(n,p)$ & $\Rightarrow$ & $E[X]=n\cdot\frac{1-p}{p}$\\
\end{tabular}

\section{Variabile aleatoria assolutamente continua}
$X$ è variabile aleatoria assolutamente continua $P(X=a)=0$
\begin{center}
    $P(X\in[a,b])=\int_{a}^{b} f_{x}(x)\,dx $
\end{center}

\textbf{Def}\\ $X$ variabile aleatoria assolutamente continua allora $E[X]=\int_{-\infty}^{+\infty}\ x\cdot f_{x}(x) \,dx$

\newpage
\textbf{Teorema}\\$X$ variabile aleatoria assolutamente continua e $Y=g(x)$ allora:
\begin{center}
    $E[Y]=\int_{\mathbb{R}} g(x) f_{x}(x)\,dx$
\end{center}

\textbf{Teorema}\\$(X,Y)$ vettlre aleatorio assolutamente continuo e $g:\mathbb{R}^{2}\rightarrow\mathbb{R}$ allora se
$Z=g(x,y)$:
\begin{center}
    $E[Z]=\int\int_{\mathcal{R}^{2}}\ g(x,y)f_{X,Y}(x,y)\,dx\,dy$
\end{center}

\textbf{Teorema}\\$(X,Y)$ vettlre aleatorio misto con $X$ discreta e $Y$ assolutamente continua e densità ibrida (o mista)
$f_{X,Y}$. Se $g:\mathbb{R}^{2}\rightarrow\mathbb{R}$ e $Z=g(X,Y)$ allora:
\begin{center}
    $E[Z]=\sum_{x\in\mathcal{R}_{x}}\int_{-\infty}^{+\infty}\ g(x,y)f_{X,Y}(x,y)\,dx\,dy$
\end{center}

\textbf{Proprietà}\\ Il valore atteso gode delle seguenti proprietà:
\begin{itemize}
    \item Linearità
    \item prodotto di variabili aleatorie indipendenti
    \item Monotonia
\end{itemize}

\section{momenti di una variabile aleatoria}

\textbf{Def}\\ per ogni $n\in\mathbb{N}/\{0\}$ si dice momento n-esimo di $X$ variabile aleatoria il 
numero reale $E[X^{n}]$.\\ Si dice \textit{momento centrale} di $X$ il numero reale $E[(X-E[X])^{n}]$\\
Il momento secondo centrale di $X$ prende il nome di \textit{varianza} di $X$: $Var[X]$

NB: la varianza misura la larghezza della distribuzione, al quadrato

\textbf{Prop}
\begin{center}
     $Var[X]=E[X^{2}]-(E[X])^{2}$
\end{center}

\textbf{Proprietà della varianza}
\begin{itemize}
    \item $Var[X]\geq0,\ Var[X]=0 \Leftrightarrow X\equiv const$
    \item $a,b\in\mathbb{R},\ Var[aX+b]=a^{2}Var[X]$
\end{itemize}

\textbf{Prop} 
\begin{center}
    Siano $X,Y$ variabili aleatorie indipendeti, allora $Var[X+Y]=Var[X]+Var[Y]$
\end{center}

\newpage
\section{Deviazione standard}
Chiamiamo \textit{deviazione standard} $\sigma_{x}$ di una variabile aleatoria $X$, il 
numero $\sigma_{x}=\sqrt{Var[X]}$\\
Sia $Y=\alpha X$ allora $\sigma_{Y}=\alpha\cdot\sigma_{X}$

\textbf{Varianza di modelli discreti noti}\\
\begin{tabular}{l c c l}
   Bernoulliane & $X\sim bin(1,p)$ & $\Rightarrow$ & $Var[X]= p(1-p)$\\
   Binomiali & $X\sim bin(n,p)$ & $\Rightarrow$ & $Var[X]= n\cdot p(1-p)$\\
   Geometriche & $X\sim geom(p)$ & $\Rightarrow$ & $Var[X]= \frac{(1-p)}{p^{2}}$\\
   Binomiali neagative & $X\sim NB(p)$ & $\Rightarrow$ & $Var[X]= n\cdot\frac{(1-p)}{p^{2}}$\\
   Poisson & $X\sim pois(\lambda)$ & $\Rightarrow$ & $Var[X]= \lambda$\\
\end{tabular}

\section{Diseguaglianze}

\textbf{Disuguaglianza di Markov}\\ Sia $X$ una variabile aleatoria non negativa, allora $\forall a>0$:
\begin{center}
    $P(X\geq a) \leq \frac{E[X]}{a}$
\end{center}

\textbf{Disuguaglianza di Chebychev}\\ Sia $X$ variabile aleatoria. Per ogni $a>0$:
\begin{center}
    $P(|X-E[X]|\geq a)\leq \frac{Var[X]}{a^{2}}$
\end{center}

Oss: posso prendere $a\sigma_{x}$ al posto di $a$ e la funzione diventa:
\begin{center}
    $P(|X-E[X]|\geq a\sigma_{x})\leq \frac{\sigma^{2}_{x}}{a^{2}\sigma_{x}^{2}}$
\end{center}

\section{Covarianza e correlazione}
Date $X,Y$ variabili aleatorie, chiamiamo \textit{covarianza} di $X$ e $Y$ il numero:
\begin{center}
    $Cpv[X,Y]=E[(X-E[X])(Y-E[Y])]$
\end{center}

Oss: Se $X=Y$ Allora
\begin{center}
    $Cov[X,Y]=E[(X-E[X])^2]=Var[X]$

    $Conv[X,Y]=E[X\cdot Y]-E[X]\cdot E[Y]$
\end{center}

\newpage
\textbf{Proprietà}
\begin{itemize}
    \item Se $X$ e $Y$ sono indipendenti allora $Cov[X,Y]=0$
    \item La covarianza è simmetrica: $Cov[X,Y]=Cov[Y,X]$
\end{itemize}

\textbf{Def}\\Date $X$ e $Y$ variabili aleatorie tali che $Cov[X,Y]=0$, allora si defininiscono
\textit{scorrelate}

\textbf{Proprietà}
\begin{itemize}
    \item $Var[X+Y]=Var[X]+Var[Y]+2\cdot Conv[X,Y]$
    \item In ogni componente la $Cov$ è lineare:
    \subitem $Cov[aX+bY,Z]=a\cdot Cov[X,Z]+b\cdot Cov[Y,Z]$ 
    \item La convarianza è bilineare:
    \subitem Dati $(a_{i})^{n}_{i=1},(b_{j})^{m}_{j=1}$ vettori reali, $(X_{i})^{n}_{i=1},
    (Y_{j})^{m}_{j=1}$ vettori aleatori:
    \subitem $Cov[\sum^{n}_{i=1} a_{i}X_{i},\sum^{m}_{j=1}b_{j}Y_{j}]=\sum_{i,j}a_{i}b_{j}\ 
    Conv[X_{i},Y_{j}]$
\end{itemize}

\textbf{Def}\\ La matrice $Cov[X_{i},Y_{i}]$ si chiama \textit{matrice di covarianza} $(\sum)$
\begin{center}
    $Cov[\vec{a}\cdot\vec{X},\vec{b}\cdot\vec{Y}]=\vec{a}^{t}\sum\vec{b}$
\end{center}

\textbf{Prop}
\begin{center}
    $-\sqrt{Var[X]Var[Y]}\leq Cov[X,Y]\leq\sqrt{Var[X]Var[Y]}$
\end{center}

\textbf{Def}\\Date $X$ e $Y$ variabili aleatorie, chiamiamo \textit{Correlazione} il numero:
\begin{center}
    $\mathcal{P}(X,Y)=Corr[X,Y]=\frac{Cov[X,Y]}{\sqrt{Var[X]Var[Y]}}$
\end{center}


\section{Mediane}\marginpar{06/05/21}
Si dice \textit{Mediane} di una variabile aleatoria $X$ un numero $mx$ tale che:
\begin{center}
    $P(X\leq mx)=P(X\geq mx)$
\end{center}

\newpage
Oss:
\begin{center}
    $P(X\leq mx)=F_{X}(mx)$\\
    $P(X\geq mx)=1-F_{X}(mx)-P(X=mx)$
\end{center}
cioé, per $X$ assolutamente continua $mx$ tale che:
\begin{center}
    $F_{X}(mx)=1-F_{X}(mx)$ ossia $F_{X}(mx)=1/2\qquad mx\in F_{X}^{-1}(\{1/2\})$
\end{center}
Per $X$ assolutamente continua esiste una mediana, ma può essere non unica

Oss: Se $X$ è  una variabile aleatoria discreat, la mediana può non essere unica o non esistere

\textbf{Def}\\ Chiamiamo mediana impropria un reale $\widetilde{m}x$ tale che:
\begin{center}
    $P(X\leq\widetilde{m}x)\geq 1/2\quad$ e $\quad P(X\geq\widetilde{m}x)\geq 1/2$    
\end{center}

\section{Quantile}
Dato $X$ con legge $F_{X}$ e $p\in(1,0)$, chiamiamo \textit{p-quantile} il numero reale $Q_{X}(p)$:
\begin{center}
    $Q_{X}(p)=inf\{x\in\mathbb{R}:\ F_{X}(x)\geq p\}$
\end{center}

Oss: la funzione quantile $Q_{X}:p\rightarrow Q_{x}(p)$ $((0,1)\rightarrow\mathbb{R})$ è qualcosa di simile
all'inversa della $F_{X}$

\section{Moda}
Chiamiamo \textit{moda} di una variabile aleatoria $X$ un numero $x\in\mathcal{R}_{x}$ tale che:
\begin{itemize}
    \item Se $X$ è discreta, $\varphi_{x}$ è massima in $x$, cioé $x\in argmax\ \varphi_{x}(y)$
    \item Se $X$ è continua, $f{x}$ è massima in $x$, cioé $x\in argmax\ f_{x}(y)$
\end{itemize}

Se la moda è unica $X$ è unimodale. Se ha 2 mode è bimodale. Se ha più di 2 mode è multimodale

\newpage
\textbf{Modelli assolutamente continui}

\section{Uniformi}
Dati due numeri reali $a<b$ chiamiamo un variabile aleatoria $X$ uniforme su $[a,b]$ se 
la sua densità $f_{X}$ è costante in $(a,b)$ e nulla altrove
\begin{center}
    $X\sim unif(a,b)\quad$ o $\quad X\sim unif[a,b]$

    $\frac{1}{b-a}$
\end{center}

Indicatori:
\begin{itemize}
    \item $E[X]=\frac{a+b}{2}$
    \item $Var[X]=\frac{(b-a)^{2}}{12}$
\end{itemize}
la mediana coincide con la media e la moda coincide con qualunque valore in $(a,b)$

\section{Esponenziali}
$X$ è esponenziale di parametro $\lambda>0$ se
\begin{center}
    $f_{x}(x)=\begin{cases}
        c\cdot e^{-\lambda x} & x\geq 0\\
        0 & x<0
    \end{cases}$
\end{center}
$c=\lambda$, $F_{X}(x)=1-e^{-\lambda x}$

Indicatori
\begin{itemize}
    \item $E[X]=\frac{1}{\lambda}$
    \item $Var[X]=\frac{1}{\lambda^{2}}$
    \item la moda è $0$
    \item la mediana è $\frac{log(2)}{\lambda}$
\end{itemize}
Le esponenziali hanno assenza di memoria, cioé per $s,t>0$:
\begin{center}
    $P(X>s+t\mid X>s)=P(X>t)$
\end{center}

\newpage
\section{Gaussiane o Normali} $X$ è normale standard se ha densità:
\begin{center}
     $X\sim \mathcal{N}(0,1)\Rightarrow f_{x}(x)=\frac{1}{\sqrt{2\pi}}^{(1)}\cdot e^{\frac{-x^{2}}{2}\ (2)}$
\end{center}
(1) è costante di normalizzazione, mentre (2) da la forma a campana

\textbf{Proprietà}
\begin{itemize}
    \item $f_{X}$ è simmetrica rispetto a $x=0,\ f_{X}(x)=f_{X}(-x)$
    \item $f_{x}$ ha massimo in $x=0$. Tale massimo è $\frac{1}{\sqrt{2\pi}}$
    \item ha punti di flesso in $\pm 1$
    \item in $\pm 2$ ha valore $\approx 0.05$ e in $\pm 3$ vale $\approx 0.004$
\end{itemize}

\textbf{Funzione di ripartizione}
\begin{center}
    $\Phi(x)=\int_{-\infty}^{\infty}\ \frac{1}{\sqrt{2\pi}}\cdot e^{\frac{-x^{2}}{2}} dt$ 
\end{center}

\textbf{Proprietà}
\begin{itemize}
    \item in $0$ vale $\frac{1}{2}$
    \item è simmetrica rispetto a $(0,\frac{1}{2})\rightarrow \Phi(x)=1-\Phi(x)$
    \item in $-3$ vale $\approx 0.0013$, in $3$ vale $\approx 0.9987$
    \item $\Phi(-2)\approx 0.0228$, $\Phi(2)\approx 0.9772$
    \item è monotona strettamente crescente
\end{itemize}

$\Phi^{-1}$ è funzione quantile
\begin{center}
    $\Phi^{-1}(p)=x\Leftrightarrow\Phi(x)=p\Leftrightarrow P(X\leq x)=p$
\end{center}
La funzione quantile è simmetrica rispetto a $(\frac{1}{2},p)$, cioé $\Phi^{-1}(p)=-\Phi^{-1}(1-p)$

Indicatori
\begin{itemize}
    \item $E[X]=0$
    \item $Var[X]=1$
\end{itemize}

\textbf{Def}\\ Sia $Z\sim\mathcal{N}(0,1)$ allora $X$ è una variabile aleatoria normale di parametri
$\mu\in\mathbb{R},\sigma>0$ se $X=\sigma Z+\mu$, e scriveremo $X\sim\mathcal{N}(\mu,\sigma)$
\begin{center}
    $F_{X}(x)=P(z\leq \frac{x-\mu}{\sigma})=\Phi(\frac{x-\mu}{\sigma})$

    $f_{X}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\cdot e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}$
\end{center}
\begin{itemize}
    \item $E[X]=\mu$
    \item $Var[X]=\sigma^{2}$
\end{itemize}

\textbf{Proprietà}\\ $X\sim\mathcal{N}(\mu,\sigma)$ eredità le proprietà di $Z\sim\mathcal{N}(0,1)$
tenendo conto di trasformazioni e deviazione

\textbf{Prop:} la famiglia  Gaussiana è riproducibile. Dati $X_{1}\sim\mathcal{N}(\mu_{1},\sigma_{1})$
e $X_{2}\sim\mathcal{N}(\mu_{2},\sigma_{2})$, allora:
\begin{center}
    $X_{1}+X_{2}\sim\mathcal{N}(\mu_{1}+\mu_{2},\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}})$
\end{center}

\section{Chi quadro}
Se $X$ è una somma di $n$ quadrati di guassiane standard indipendenti, diciamo
che $X$ è una chi quadro con $n$ gradi di libertà, e scriveremo $X\sim X^{2}_{n}$ oppure 
$X\sim\mathcal{X}^{2}(n)$
\begin{center}
    $(Y_{i})^{n}_{i=1}\quad iid\qquad Y_{i}\sim\mathcal{N}(0,1)\quad X=\sum_{i=1}^{n}\ Y_{i}^{2}$
\end{center}

Oss: Se sommiamo $2$ quadrati, la distribuzione che otteniamo è una esponenziale con 
$\lambda=\frac{1}{2}$

Oss: le chi quadro sono riproducibili

La densità $f_{X}(x)=c_{n}\cdot x^{\frac{n}{2}-1}\cdot e^{\frac{-x}{2}}$ con $c_{n}$ opportuna
rinormalizzazione

Indicatori:
\begin{itemize}
    \item $E[X]=n$
    \item $Var[X]=2n$
\end{itemize}

\section{T (di Student)}
Data $Z\sim\mathcal{N}(0,1),\ W\sim \mathcal{X}^{2}(n)$ indipendenti, $X$ è una \textit{t di student}
a $n$ gradi di libertà se $X=\frac{Z}{\sqrt{w/n}}$ e scriviamo $X\sim t(n)$ o $X\sim t_{n}$

\textbf{Proprietà}
\begin{itemize}
    \item $f_{X}(x)=f_{X}(-x)$
    \item $F_{X}(-x)=1-F_{X}(x)$
    \item $F^{-1}_{X}(p)=-F^{-1}_{X}(1-p)$
\end{itemize}

Indicatori
\begin{itemize}
    \item $E[X]=0\quad \forall n$
    \item $Var[X]=\begin{cases}
        $non definita$ & n=1\\
        +\infty & n=2\\
        \frac{n}{n-2} & n>2
    \end{cases}$
\end{itemize}
Oss: $Var[X]\rightarrow 1$ con lim $ n\to \infty$
\begin{itemize}
    \item $E[\frac{W}{n}]=1$
    \item $Var[\frac{W}{n}]=\frac{2}{n}\qquad n\to\infty=0$
\end{itemize}

\marginpar{17/05/21}
\section{Convergenza variabili aleatorie}
\subsection{convergenza quasi certa}
$(\Omega,\mathcal{F},P)$ spazio di probabilità e $X$ variabile aleatoria su di esso e $(X_{n})_{n\in\mathbb{N}}$
una successione di variabili aleatorie sullo spazio di probabilità. Diciamo che $(X_{n})_{n}$ \textit{converge
quasi certamente} o \textit{puntualmente} a $X$ e scriviamo $X_{n}\xrightarrow[n\to\infty]{\text{qc}}$
se esiste $E\in\mathcal{F}$ con $P(E)=1$ tale che $\forall w\in E$:
\begin{center}
    $\lim_{n\to\infty}\quad X_{n}(w)=X(w)$
\end{center}

\subsection{Convergenza in probabilità}
$(\lambda_{n})_{n}$ successione di variabili aleatorie e $X$ variabile aleatoria su $(\Omega,\mathcal{F},P)$.
Diciamo che $(X_{n})_{n}$ converge \textit{in probabilità} a $X$ e scriviamo $X_{n}\xrightarrow[n\to\infty]{p}X$
se $\forall\varepsilon>0$:
\begin{center}
    $\lim_{n\to\infty}\quad P(|X_{n}-X|>\varepsilon)=0 $
\end{center}

\subsection{Convergenza in media quadratica}
Diciamo che $(X_{n})_{n}$ converge in \textit{media quadratica} on in $\mathcal{L}^{2}$ a $X$ e scriviamo
$X_{n}\xrightarrow[n\to\infty]{\mathcal{L}^{2}}X$ se:
\begin{center}
    $\lim_{n\to\infty}\quad E[|X_{n}-X|^{2}]=0$
\end{center}

Prop: la convergenza in media quadratica implica la convergenza in probabilità, cioé
\begin{center}
    $X_{n}\xrightarrow[n\to\infty]{\mathcal{L}^{2}}X\qquad$ allora $\qquad X_{n}\xrightarrow[n\to\infty]{p}X$
\end{center}

\subsection{Convergenza in distribuzione}
Sia $(X_{n})_{n\in\mathbb{N}}$ successione su $(\Omega,\mathcal{F},P)$ e $X$ variabile aleatoria su $(\tilde{\Omega}
,\tilde{\mathcal{F}},\tilde{P})$. Diciamo che $(X_{n})_{n}$ converge \textit{in distribuzione} o \textit{in legge}
o \textit{debolmente} a $X$ ($X_{n}\xrightarrow[n\to\infty]{\mathfrak{L}}X,\ 
X_{n}\xrightarrow[n\to\infty]{d}X\\o\ X_{n}\xrightarrow[n\to\infty]{w}X$) se $\forall x\in\mathbb{R}$:
\begin{center}
    $\lim_{n\to\infty}\quad P(X_{n}\leq x)=P(X\leq x)$\\
    ossia\\
    $\lim_{n\to\infty}\quad F_{X_{n}}(x)=F_{X}(x)$
\end{center}

Prop: Le convergenze quasi certe implicano la convergenza in probabilità
\begin{center}
    $X_{n}\xrightarrow{\alpha}X\Leftarrow X_{n}\xrightarrow{p}X\Leftarrow
    \begin{cases}
        X_{n}\xrightarrow{qc}X\\
        X_{n}\xrightarrow{\mathcal{L}^{2}}X    
    \end{cases}$
\end{center}

Oss: La convergenza in $\mathcal{L}^{2}$ e $qc$ non sono confrontabili

Prop: la convergenza in probabilità implica la convergenza debolmente

\section{Teoremi limite}
Sia $(X_{1}\_\_ X_{n})$ un vettore aleatorio con componenti indipendenti, di media comune $\mu$ e varianza comune
$\sigma^{2}$. Sia $S_{n}=\sum_{i=1}^{n}\ X$ la variabile aleatoria somma. Allora:
\begin{center}
    $E[\frac{S_{n}}{n}]=\mu\quad$ e $\quad Var[\frac{-S_{n}}{n}]=\frac{\sigma^{2}}{n}$
\end{center}

\section{Teorema della legge debole dei grandi numeri}
Sia $(X_{n})_{n\in\mathbb{N}}$ variabile aleatoria ciascuna di media $\mu$ e  varianza $\sigma^{2}$. Sia inoltre
$S_{n}=\sum_{i=1}^{n}\ X_{i}$. Allora $\frac{S_{n}}{n}$ converge in probabilità a $\mu$, cioé $\forall\varepsilon
>0$:
\begin{center}
    $\lim_{n\to\infty}\quad P(|\frac{S_{n}}{n}-\mu|>\varepsilon)=0$
\end{center}

\section{Teorema centrale del limite}
\begin{center}
    $\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\xrightarrow{\mathfrak{L}}\mathcal{N}(0,1)$\\
    cioé\\
    $\lim_{n\to\infty}\quad P(\frac{S-n\mu}{\sigma\sqrt{n}}\leq x)=\Phi(x)$
\end{center}

Oss: $\frac{S_{n}-n\mu}{\sigma}$ è nell'ordine di $\sqrt{n}$ per $n\to\infty$

Oss: il modo in cui usiamo il TLC se $n$ è sufficientemente grande, allora:
\begin{center}
    $\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\sim\mathcal{N}(0,1)\qquad\frac{S_{n}}{2}\sim\mathcal{N}(\mu,
    \frac{\sigma}{\sqrt{n}})$

    $S_{n}\sim\mathcal{N}(n\mu,\sigma\sqrt{n})$
\end{center}
$n$ è sufficientemente grande quando:
\begin{itemize}
    \item $X_{i}\sim\mathcal{N}\to\ n\geq 1$
    \item $X_{i}\sim Unif\to\ n\geq 5$
    \item $X_{i}sim geom$ o $X_{i}\sim exp\to\ n\geq 15$
    \item $X_{i}\sim X^{2}\to n\geq 25$
\end{itemize}

\subsection{Correzione di continuità}
Se le $X_{i}$ sono discrete nella versione approssimata usiamo la correzione di continuita, ossia:
\begin{center}
    $F_{S_{n}}(x)\simeq\Phi\left(\frac{x+1/2-n\cdot E[X_{i}]}{\sqrt{n\cdot Var[X_{i}]}}\right)$
\end{center}

Oss:$bin(n,p)\sim\mathcal{N}(np,\sqrt{np(1-p)})$ purché $p$ sia lontano da 0 e 1. Cioé se $np(1-p)\gtrsim 3$:
\begin{center}
    $Pois(\lambda)\sim\mathcal{N}(\lambda,\sqrt\lambda)\qquad$ per $\lambda\geq 30$
\end{center}

\newpage

\begin{center}
    \textbf{Statistica}
\end{center}

\section{Popolazione di riferiento}
La \textit{popolazione di riferimento} è un insieme di elementi sui quali conduciamo un'indagine. Gli elemtni si
chiamano \textit{individui, esemplari} o \textit{unità statistiche}.

Il \textit{Campione} è un sottoinsieme della popolazione

\section{Campionamento}
Esistono dievrsi tipi di campionamento: \textit{campionamento casuale semplice, campionamento stratificato, 
campionamento a grappoli}

\section{Variabili}
Le caratteristiche che misuriamo sono definite \textit{variabili}. I loro valori si chiamano \textit{modalità} o
\textit{livelli}:
\begin{itemize}
    \item qualitative o categoriche
    \subitem nominali (senza un ordine naturale)
    \subitem ordinali (con ordine naturale)
    \item quantitative o numeriche
    \subitem discrete
    \subitem continue
\end{itemize}
Le variabili quantitative possono essere quantificate in scale per intervallo o rapporto

\section{Statistica}
Chiamiamo \textit{statistica} una funzione calcolabile dalle misurazioni del campione.

Si dice \textit{stimatore} di un parametro di una variabile aleatoria discreta che sia una statistica e il cui
valore sia "spesso vicino" al parametro d'interesse.\\ Il valore deterministico assunto dallo stimatore usando la
realizzazione del campione si chiama \textit{stima} del parametro

\textbf{Notazione:} Se $\theta$ è uno stimatore di $\vartheta$, possiamo scrivere $\theta_{n}$ per evidenziare la 
numerosità del campione e $\hat{\vartheta}=\theta$.\\ Chiedere che $\theta$ sia vicino a $\vartheta$ significa che 
l'errore di stima sia piccolo

\textbf{Def}\\Uno stimatote $\theta$ di $\vartheta$ è:
\begin{itemize}
    \item \textit{corretto} o \textit{non distorto} se $E[\theta]=\vartheta$
    \item \textit{distorto} se $E[\theta]\neq\vartheta$. Chiameremo allora \textit{bias} $E[\theta]-\vartheta$
\end{itemize}
Se $Lim_{n\to\infty}\ E[\theta_{n}]=\vartheta$ allora $\theta$ è \textit{asintoticamente non distorto}

\section{Errore quadratico medio}
Si dice \textit{Errore quadratrico medio}, o MSE, di $\theta$ la quantità $MSE[\theta]=E[(\theta-\vartheta)^{2}]$:
\begin{center}
    $MSE[\theta]=Var[\theta]+bias^{2}$
\end{center}

\textbf{Def}\\$\theta$ è \textit{consistente} se $\theta_{n}\xrightarrow{p}\theta$. $\theta$ è \textit{consistente
in media quadratica} se $\theta_{n}\xrightarrow{\mathcal{L}^{2}}\theta$

\textbf{Prop}\\Se $\theta$ è asintoticamente non distorto e $lim_{n\to\infty}\ Var[\theta_{n}]=0$ allora $\theta$
è consistente in media quadratica e quindi consistente

Oss\\ Stimatori corretti ma non consistenti $(X_{1},...,X_{n})$. Allora una qualunque $X_{i}$ è stimatore della
media $\mu\Rightarrow E[X_{i}]=\mu$. Se $\sigma^{2}=Var[X_{i}]\neq 0$ allora $P(|X_{i}-\mu|>\varepsilon)>0$ e 
rimane tale per $n\to\infty\Rightarrow$ non conv in $P$

\end{document}